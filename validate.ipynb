{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to respect the following constraints:\n",
    "\n",
    "- The separator between the figure ID and the concepts has to be a tabular whitespace\n",
    "\n",
    "- The separator between the UMLS concepts has to be a semicolon (;)\n",
    "\n",
    "- The same concept cannot be specified more than once for a given figure ID\n",
    "\n",
    "- Each figure ID of the testset must be included in the submission file exactly once (even if there are no concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the separator between the figure ID and the concepts has to be a tabular whitespace.\n",
    "results = pandas.read_csv(filepath_or_buffer='results.csv', delimiter='\\t', names=['figure', 'concepts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = pandas.read_csv(filepath_or_buffer='testing.csv', delimiter='\\t', names=['figure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>figure</th>\n",
       "      <th>concepts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fsurg-04-00047-g002</td>\n",
       "      <td>C0016538;C0033363;C0221055;C0262878;C0309989;C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AJC-17-412-g001</td>\n",
       "      <td>C0006290;C0014245;C0040578;C0040580;C0043246;C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gr7_PMC5545870</td>\n",
       "      <td>C0014245;C0034196;C0038351;C0038354;C0153418;C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SaudiMedJ-38-541-g004</td>\n",
       "      <td>C0009924;C0027651;C0030797;C0034606;C0040405;C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ijn-12-2179Fig1</td>\n",
       "      <td>C0005889;C0017547;C0041618;C0043194;C0221055;C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  figure                                           concepts\n",
       "0    fsurg-04-00047-g002  C0016538;C0033363;C0221055;C0262878;C0309989;C...\n",
       "1        AJC-17-412-g001  C0006290;C0014245;C0040578;C0040580;C0043246;C...\n",
       "2         gr7_PMC5545870  C0014245;C0034196;C0038351;C0038354;C0153418;C...\n",
       "3  SaudiMedJ-38-541-g004  C0009924;C0027651;C0030797;C0034606;C0040405;C...\n",
       "4        ijn-12-2179Fig1  C0005889;C0017547;C0041618;C0043194;C0221055;C..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the separator between the UMLS concepts has to be a semicolon (;).\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>figure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12864_2017_3726_Fig1_HTML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13071_2017_2412_Fig3_HTML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13071_2017_2412_Fig4_HTML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41598_2017_1378_Fig2_HTML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41598_2017_1378_Fig3_HTML</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      figure\n",
       "0  12864_2017_3726_Fig1_HTML\n",
       "1  13071_2017_2412_Fig3_HTML\n",
       "2  13071_2017_2412_Fig4_HTML\n",
       "3  41598_2017_1378_Fig2_HTML\n",
       "4  41598_2017_1378_Fig3_HTML"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>figure</th>\n",
       "      <th>concepts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9938</td>\n",
       "      <td>9938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>9938</td>\n",
       "      <td>9775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>bi-2017-003002_0008</td>\n",
       "      <td>C0221055;C1550557;C1706368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     figure                    concepts\n",
       "count                  9938                        9938\n",
       "unique                 9938                        9775\n",
       "top     bi-2017-003002_0008  C0221055;C1550557;C1706368\n",
       "freq                      1                          47"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each figure ID of the testset must be included in the submission file exactly once (even if there are no concepts).\n",
    "results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>figure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>9938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>bi-2017-003002_0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     figure\n",
       "count                  9938\n",
       "unique                 9938\n",
       "top     bi-2017-003002_0008\n",
       "freq                      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "figure      9938\n",
       "concepts    9938\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "figure    9938\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9938"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.figure.unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9938"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing.figure.unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(results['figure']) - set(testing['figure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(testing['figure']) - set(results['figure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same concept cannot be specified more than once for a given figure ID.\n",
    "for figure, concepts in results.values:\n",
    "    tmp = concepts.split(';')\n",
    "    total = len(tmp)\n",
    "    unique = len(set(tmp))\n",
    "    if total != unique:\n",
    "        print(total, unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): sklearn in /opt/conda/lib/python3.5/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): scikit-learn in /opt/conda/lib/python3.5/site-packages (from sklearn)\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied (use --upgrade to upgrade): scipy in /opt/conda/lib/python3.5/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy>=1.8.2 in /opt/conda/lib/python3.5/site-packages (from scipy)\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, argparse, string\n",
    "import csv\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(candidate_file, gt_file):\n",
    "\n",
    "    # Hide warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    # Concept stats\n",
    "    min_concepts = sys.maxsize\n",
    "    max_concepts = 0\n",
    "    total_concepts = 0\n",
    "    concepts_distrib = {}\n",
    "\n",
    "#     # Parse arguments\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('candidate_file', help='path to the candidate file to evaluate')\n",
    "#     parser.add_argument('gt_file', help='path to the ground truth file')\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    # Read files\n",
    "    print('Input parameters\\n********************************')\n",
    "\n",
    "    print('Candidate file is \"' + candidate_file + '\"')\n",
    "    candidate_pairs = readfile(candidate_file)\n",
    "\n",
    "    print('Ground Truth file is \"' + gt_file + '\"')\n",
    "    gt_pairs = readfile(gt_file)\n",
    "\n",
    "    # Define max score and current score\n",
    "    max_score = len(gt_pairs)\n",
    "    current_score = 0\n",
    "\n",
    "    # Check there are the same number of pairs between candidate and ground truth\n",
    "    if len(candidate_pairs) != len(gt_pairs):\n",
    "        print('ERROR : Candidate does not contain the same number of entries as the ground truth!')\n",
    "        exit(1)\n",
    "\n",
    "    # Evaluate each candidate concept list against the ground truth\n",
    "    print('Processing concept sets...\\n********************************')\n",
    "\n",
    "    i = 0\n",
    "    for image_key in candidate_pairs:\n",
    "\n",
    "        # Get candidate and GT concepts\n",
    "        candidate_concepts = candidate_pairs[image_key].upper()\n",
    "        gt_concepts = gt_pairs[image_key].upper()\n",
    "\n",
    "        # Split concept string into concept array\n",
    "        # Manage empty concept lists\n",
    "        if gt_concepts.strip() == '':\n",
    "            gt_concepts = []\n",
    "        else:\n",
    "            gt_concepts = gt_concepts.split(',')\n",
    "\n",
    "        if candidate_concepts.strip() == '':\n",
    "            candidate_concepts = []\n",
    "        else:\n",
    "            candidate_concepts = candidate_concepts.split(',')\n",
    "\n",
    "        # Manage empty GT concepts (ignore in evaluation)\n",
    "        if len(gt_concepts) == 0:\n",
    "            max_score -= 1\n",
    "        # Normal evaluation\n",
    "        else:\n",
    "            # Concepts stats\n",
    "            total_concepts += len(gt_concepts)\n",
    "\n",
    "            # Global set of concepts\n",
    "            all_concepts = sorted(list(set(gt_concepts + candidate_concepts)))\n",
    "\n",
    "            # Calculate F1 score for the current concepts\n",
    "            y_true = [int(concept in gt_concepts) for concept in all_concepts]\n",
    "            y_pred = [int(concept in candidate_concepts) for concept in all_concepts]\n",
    "\n",
    "            f1score = f1_score(y_true, y_pred, average='binary')\n",
    "\n",
    "            # Increase calculated score\n",
    "            current_score += f1score\n",
    "\n",
    "        # Concepts stats\n",
    "        nb_concepts = str(len(gt_concepts))\n",
    "        if nb_concepts not in concepts_distrib:\n",
    "            concepts_distrib[nb_concepts] = 1\n",
    "        else:\n",
    "            concepts_distrib[nb_concepts] += 1\n",
    "\n",
    "        if len(gt_concepts) > max_concepts:\n",
    "            max_concepts = len(gt_concepts)\n",
    "\n",
    "        if len(gt_concepts) < min_concepts:\n",
    "            min_concepts = len(gt_concepts)\n",
    "\n",
    "        # Progress display\n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print(i, '/', len(gt_pairs), ' concept sets processed...')\n",
    "\n",
    "    # Print stats\n",
    "    print('Concept statistics\\n********************************')\n",
    "    print('Number of concepts distribution')\n",
    "    print_dict_sorted_num(concepts_distrib)\n",
    "    print('Least concepts in set :', min_concepts)\n",
    "    print('Most concepts in set :', max_concepts)\n",
    "    print('Average concepts in set :', total_concepts / len(candidate_pairs))\n",
    "\n",
    "    # Print evaluation result\n",
    "    print('Final result\\n********************************')\n",
    "    print('Obtained score :', current_score, '/', max_score)\n",
    "    print('Mean score over all concept sets :', current_score / max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read a Tab-separated ImageID - Caption pair file\n",
    "def readfile(path):\n",
    "    try:\n",
    "        pairs = {}\n",
    "        with open(path) as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "            for row in reader:\n",
    "                # We have an ID and a set of concepts (possibly empty)\n",
    "                if len(row) == 2:\n",
    "                    pairs[row[0]] = row[1]\n",
    "                # We only have an ID\n",
    "                elif len(row) == 1:\n",
    "                    pairs[row[0]] = ''\n",
    "                else:\n",
    "                    print('File format is wrong, please check your run file')\n",
    "                    exit(1)\n",
    "\n",
    "        return pairs\n",
    "    except FileNotFoundError:\n",
    "        print('File \"' + path + '\" not found! Please check the path!')\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print 1-level key-value dictionary, sorted (with numeric key)\n",
    "def print_dict_sorted_num(obj):\n",
    "    keylist = [int(x) for x in list(obj.keys())]\n",
    "    keylist.sort()\n",
    "    for key in keylist:\n",
    "        print(key, ':', obj[str(key)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input parameters\n",
      "********************************\n",
      "Candidate file is \"results.csv\"\n",
      "Ground Truth file is \"results.csv\"\n",
      "Processing concept sets...\n",
      "********************************\n",
      "1000 / 9938  concept sets processed...\n",
      "2000 / 9938  concept sets processed...\n",
      "3000 / 9938  concept sets processed...\n",
      "4000 / 9938  concept sets processed...\n",
      "5000 / 9938  concept sets processed...\n",
      "6000 / 9938  concept sets processed...\n",
      "7000 / 9938  concept sets processed...\n",
      "8000 / 9938  concept sets processed...\n",
      "9000 / 9938  concept sets processed...\n",
      "Concept statistics\n",
      "********************************\n",
      "Number of concepts distribution\n",
      "1 : 9938\n",
      "Least concepts in set : 1\n",
      "Most concepts in set : 1\n",
      "Average concepts in set : 1.0\n",
      "Final result\n",
      "********************************\n",
      "Obtained score : 9938.0 / 9938\n",
      "Mean score over all concept sets : 1.0\n"
     ]
    }
   ],
   "source": [
    "main('results.csv', 'results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n---------------\\nIMPORTANT LINKS\\n---------------\\n\\nhttp://www.crowdai.org/challenges/imageclef-2018-caption-concept-detection/submissions\\nhttp://ceur-ws.org/Vol-1866/\\nhttp://clef2018.clef-initiative.eu\\nhttp://imageclef.org/2018\\nhttp://imageclef.org/2018/caption\\nhttp://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "---------------\n",
    "IMPORTANT LINKS\n",
    "---------------\n",
    "\n",
    "http://www.crowdai.org/challenges/imageclef-2018-caption-concept-detection/submissions\n",
    "http://ceur-ws.org/Vol-1866/\n",
    "http://clef2018.clef-initiative.eu\n",
    "http://imageclef.org/2018\n",
    "http://imageclef.org/2018/caption\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTitle:\\n\\nConcept Detection on Medical Images using Deep Learning with Depthwise Separable Convolutions.\\n\\nDescription:\\n\\nConvolutional neural networks are considered among the best classifiers for single-label image classification. In this task, we adapt a convolutional neural network with transfer learning to multi-label classification task. In particular, we use the Xception, a novel deep convolutional neural network architecture inspired by the Inception network. Inception was primarily designed for the ImageNet competition and is characterized by depthwise separable convolutions.\\n\\nThe CNN architecture of Xception is based entirely on depthwise separable convolution layers and has 36 convolutional layers forming the feature extraction base of the network. Since this is a image classification task, the convolutional base here is followed by a logistic regression layer. In addition, we have insert fully-connected layers before the logistic regression layer, which is explored in the system presentation section. The 36 convolutional layers are structured into 14 modules, all of which have linear residual connections around them, except for the first and last modules.\\n\\nIn order to train the CNN model we used Jupyter Notebook and Keras on Amazon AWS through the AWS Deep Learning AMI. Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Keras is a deep learning framework and high-level library that uses TensorFlow as its back-end engine. Finally, the AWS Deep Learning AMIs provide machine learning practitioners and researchers with the infrastructure and tools to accelerate deep learning in the cloud, at any scale.\\n\\nThe overall process, which includes both training and testing, took about 24 hours for both phases combined and our final model was able to achieve an F1-Score of 22.06% on the validation data.\\n\\nRetrieval Type:\\n\\nMixed (Textual and Visual)\\n\\nRun Type:\\n\\nAutomatic\\n\\nPrimary Run?\\n\\nChecked\\n\\nOther information:\\n\\nNothing to add here.\\n\\nAdditional resources used\\n\\nNo additional resources were used.\\n\\nChoose File:\\n\\n/home/eualin/Desktop/results.csv\\n\\nhttps://www.crowdai.org/challenges/imageclef-2018-caption-concept-detection/submissions\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Title:\n",
    "\n",
    "Concept Detection on Medical Images using Deep Learning with Depthwise Separable Convolutions.\n",
    "\n",
    "Description:\n",
    "\n",
    "Convolutional neural networks are considered among the best classifiers for single-label image classification. In this task, we adapt a convolutional neural network with transfer learning to multi-label classification task. In particular, we use the Xception, a novel deep convolutional neural network architecture inspired by the Inception network. Inception was primarily designed for the ImageNet competition and is characterized by depthwise separable convolutions.\n",
    "\n",
    "The CNN architecture of Xception is based entirely on depthwise separable convolution layers and has 36 convolutional layers forming the feature extraction base of the network. Since this is a image classification task, the convolutional base here is followed by a logistic regression layer. In addition, we have insert fully-connected layers before the logistic regression layer, which is explored in the system presentation section. The 36 convolutional layers are structured into 14 modules, all of which have linear residual connections around them, except for the first and last modules.\n",
    "\n",
    "In order to train the CNN model we used Jupyter Notebook and Keras on Amazon AWS through the AWS Deep Learning AMI. Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Keras is a deep learning framework and high-level library that uses TensorFlow as its back-end engine. Finally, the AWS Deep Learning AMIs provide machine learning practitioners and researchers with the infrastructure and tools to accelerate deep learning in the cloud, at any scale.\n",
    "\n",
    "The overall process, which includes both training and testing, took about 24 hours for both phases combined and our final model was able to achieve an F1-Score of 22.06% on the validation data.\n",
    "\n",
    "Retrieval Type:\n",
    "\n",
    "Mixed (Textual and Visual)\n",
    "\n",
    "Run Type:\n",
    "\n",
    "Automatic\n",
    "\n",
    "Primary Run?\n",
    "\n",
    "Checked\n",
    "\n",
    "Other information:\n",
    "\n",
    "Nothing to add here.\n",
    "\n",
    "Additional resources used\n",
    "\n",
    "No additional resources were used.\n",
    "\n",
    "Choose File:\n",
    "\n",
    "/home/eualin/Desktop/results.csv\n",
    "\n",
    "https://www.crowdai.org/challenges/imageclef-2018-caption-concept-detection/submissions\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
