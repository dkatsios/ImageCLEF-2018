{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source activate tensorflow_p36\n",
    "# pip install sklearn losswise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "#from cv2 import imread, resize\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import random\n",
    "import calendar\n",
    "import time\n",
    "from random import uniform\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import numpy as np\n",
    "from keras.applications.xception import Xception\n",
    "from keras.layers import Input, Conv2D, merge, Dense, Flatten, MaxPooling2D, GlobalAveragePooling2D, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from losswise.libs import LosswiseKerasCallback\n",
    "import losswise\n",
    "losswise.set_api_key('JWN8A6X96')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.image_data_format() == 'channels_last'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_path = '/home/ubuntu/Dimitris/'\n",
    "data_path = '/home/ubuntu/data/'\n",
    "images_path = '/home/ubuntu/temp/images/jpeg/'\n",
    "csv_path = data_path + 'ConceptDetectionTraining2018-Concepts.csv'\n",
    "\n",
    "results_path = save_path + r'results/'\n",
    "val_size = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create and save lists and dicts"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def make_name_labels_dict(csv_path):\n",
    "    csv_name_labels_dict = dict()\n",
    "    names_list = []\n",
    "    labels_list = set()\n",
    "    with open(csv_path) as f:\n",
    "        f.readline()\n",
    "        for line in f:\n",
    "            l = line.strip().split('\\t')\n",
    "            if len(l) != 2:\n",
    "                print(l)\n",
    "                continue\n",
    "            name, labels = l[0], l[1].strip().split(';')\n",
    "            names_list.append(name)\n",
    "            labels_list |= set(labels)\n",
    "            csv_name_labels_dict[name] = labels\n",
    "        labels_list = sorted(list(labels_list))\n",
    "    return csv_name_labels_dict, names_list, labels_list\n",
    "\n",
    "csv_name_labels_dict, names_list, labels_list = make_name_labels_dict(csv_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "save_obj(csv_name_labels_dict, save_path + 'csv_name_labels_dict')\n",
    "save_obj(names_list, save_path + 'names_list')\n",
    "save_obj(labels_list, save_path + 'labels_list')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_names_list = names_list[val_size:]\n",
    "train_name_labels_dict = dict()\n",
    "\n",
    "for name in train_names_list:\n",
    "    train_name_labels_dict[name] = csv_name_labels_dict[name]\n",
    "\n",
    "val_names_list = names_list[:val_size]\n",
    "val_name_labels_dict = dict()\n",
    "\n",
    "for name in val_names_list:\n",
    "    val_name_labels_dict[name] = csv_name_labels_dict[name]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "save_obj(train_names_list, save_path + 'train_names_list')\n",
    "save_obj(train_name_labels_dict, save_path + 'train_name_labels_dict')\n",
    "save_obj(val_names_list, save_path + 'val_names_list')\n",
    "save_obj(val_name_labels_dict, save_path + 'val_name_labels_dict')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load lists and dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_name_labels_dict = load_obj(save_path + 'csv_name_labels_dict')\n",
    "names_list = load_obj(save_path + 'names_list')\n",
    "labels_list = load_obj(save_path + 'labels_list')\n",
    "\n",
    "train_names_list = load_obj(save_path + 'train_names_list')\n",
    "train_name_labels_dict = load_obj(save_path + 'train_name_labels_dict')\n",
    "\n",
    "val_names_list = load_obj(save_path + 'val_names_list')\n",
    "val_name_labels_dict = load_obj(save_path + 'val_name_labels_dict')\n",
    "\n",
    "labels_num = len(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    \"\"\"Computes the F score.\n",
    "\n",
    "    The F score is the weighted harmonic mean of precision and recall.\n",
    "    Here it is only computed as a batch-wise average, not globally.\n",
    "\n",
    "    This is useful for multi-label classification, where input samples can be\n",
    "    classified as sets of labels. By only using accuracy (precision) a model\n",
    "    would achieve a perfect score by simply assigning every class to every\n",
    "    input. In order to avoid this, a metric should penalize incorrect class\n",
    "    assignments as well (recall). The F-beta score (ranged from 0.0 to 1.0)\n",
    "    computes this, as a weighted mean of the proportion of correct class\n",
    "    assignments vs. the proportion of incorrect class assignments.\n",
    "\n",
    "    With beta = 1, this is equivalent to a F-measure. With beta < 1, assigning\n",
    "    correct classes becomes more important, and with beta > 1 the metric is\n",
    "    instead weighted towards penalizing incorrect class assignments.\n",
    "    \"\"\"\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "\n",
    "def fmeasure(y_true, y_pred):\n",
    "    \"\"\"Computes the f-measure, the harmonic mean of precision and recall.\n",
    "\n",
    "    Here it is only computed as a batch-wise average, not globally.\n",
    "    \"\"\"\n",
    "    return fbeta_score(y_true, y_pred, beta=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_generator(this_name_labels_dict, this_names_list,\n",
    "                    batch_size, img_size=(224, 224, 3), suffix='.jpg'):\n",
    "    while True:\n",
    "        imgs = np.zeros((batch_size, *img_size))\n",
    "        labels = np.zeros((batch_size, labels_num))\n",
    "        i = 0\n",
    "        while i < batch_size:\n",
    "            name = random.choice(this_names_list)\n",
    "            filename = images_path + name + suffix\n",
    "            img = load_img(filename, target_size=img_size[:2])\n",
    "            img = img_to_array(img)\n",
    "            img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
    "            img = preprocess_input(img, mode='tf')\n",
    "            \n",
    "            # flip horizontaly\n",
    "            if uniform(0, 1) > 0.5:\n",
    "                img = np.fliplr(img)\n",
    "            \n",
    "            # add noise\n",
    "            if uniform(0, 1) > 0.5:\n",
    "                noise = np.random.normal(0, 0.05, img.shape)\n",
    "                img = img + noise\n",
    "                img = np.clip(img, -1, 1)\n",
    "            \n",
    "            imgs[i] = img\n",
    "            for label in this_name_labels_dict[name]:\n",
    "                ind = labels_list.index(label)\n",
    "                labels[i, ind] = 1\n",
    "            i += 1\n",
    "        yield imgs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "val_batch_size = 64\n",
    "\n",
    "input_shape = (224, 224, 3)\n",
    "suffix = '.jpg'\n",
    "\n",
    "train_gen = images_generator(train_name_labels_dict, train_names_list,\n",
    "                             batch_size, img_size=input_shape, suffix=suffix)\n",
    "\n",
    "val_gen = images_generator(val_name_labels_dict, val_names_list,\n",
    "                             val_batch_size, img_size=input_shape, suffix=suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.jpg'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suffix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make val arrays"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "def make_arrays(this_name_labels_dict, this_names_list):\n",
    "    imgs = np.zeros((len(this_names_list), *input_shape))\n",
    "    labels = np.zeros((len(this_names_list), labels_num))\n",
    "    \n",
    "    for ind, name in enumerate(this_names_list):\n",
    "        filename = images_path + name + suffix\n",
    "        img = load_img(filename, target_size=input_shape[:2])\n",
    "        img = img_to_array(img)\n",
    "        img = preprocess_input(img, mode='tf')\n",
    "        imgs[ind] = img\n",
    "        \n",
    "        img_labels_list = this_name_labels_dict[name]\n",
    "        for label in img_labels_list:\n",
    "            lab = labels_list.index(label)\n",
    "            labels[ind, lab] = 1\n",
    "    return imgs, labels"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# x_train, y_train = make_arrays(train_name_labels_dict, train_names_list)\n",
    "x_test, y_test = make_arrays(val_name_labels_dict, val_names_list)\n",
    "\n",
    "# save_obj(x_train, save_path + 'x_train')\n",
    "# save_obj(y_train, save_path + 'y_train')\n",
    "\n",
    "save_obj(x_test, save_path + 'x_test')\n",
    "save_obj(y_test, save_path + 'y_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x_train = load_obj(save_path + 'x_train')\n",
    "# y_train = load_obj(save_path + 'y_train')\n",
    "\n",
    "x_test = load_obj(save_path + 'x_test')\n",
    "y_test = load_obj(save_path + 'y_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create Xception model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# create the base pre-trained model\n",
    "input_tensor = Input(shape=input_shape)\n",
    "# preprocess_input = preprocess_input(input_tensor, data_forma='channels_last, mode='tf')\n",
    "base_model = Xception(input_tensor=input_tensor, weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D(name='new_GlobalAveragePooling2D')(x)\n",
    "# add a fully-connected layer\n",
    "x = Dense(1024, activation='relu', name='new_Dense_1024')(x)\n",
    "# add a logistic layer\n",
    "predictions = Dense(labels_num, name='predictions', activation='sigmoid')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "xcpetion_model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compilation and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_phase_epochs = 10\n",
    "second_phase_epochs = 10\n",
    "saves_per_epoch = 10\n",
    "small_epochs = 50\n",
    "\n",
    "imgs_per_rep = int(len(train_names_list) / saves_per_epoch)\n",
    "imgs_per_small_epoch = int(imgs_per_rep / small_epochs)\n",
    "steps_per_small_epoch = int(imgs_per_small_epoch / batch_size)\n",
    "\n",
    "steps_per_small_epoch = int(imgs_per_small_epoch / batch_size)\n",
    "\n",
    "first_phase_train_reps = first_phase_epochs * saves_per_epoch\n",
    "second_phase_train_reps = second_phase_epochs * saves_per_epoch\n",
    "\n",
    "# val_imgs_per_rep = int(val_size / saves_per_epoch)\n",
    "# val_imgs_per_small_epoch = int(val_imgs_per_rep / small_epochs)\n",
    "val_steps_per_small_epoch = int(val_size / batch_size)\n",
    "\n",
    "big_train_steps_per_epoch = int(len(train_names_list) / batch_size)\n",
    "big_val_steps_per_epoch = int(len(val_names_list) / batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f1 score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# !pip install sklearn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# import numpy as np\n",
    "# from keras.callbacks import Callback\n",
    "# from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# class Metrics(Callback):\n",
    "#     def on_train_begin(self, logs={}):\n",
    "#         self.val_f1s = []\n",
    "#         self.val_recalls = []\n",
    "#         self.val_precisions = []\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         val_predict = (np.asarray(self.model.predict(self.model.validation_data[0]))).round()\n",
    "#         val_targ = self.model.validation_data[1]\n",
    "#         _val_f1 = f1_score(val_targ, val_predict)\n",
    "#         _val_recall = recall_score(val_targ, val_predict)\n",
    "#         _val_precision = precision_score(val_targ, val_predict)\n",
    "#         self.val_f1s.append(_val_f1)\n",
    "#         self.val_recalls.append(_val_recall)\n",
    "#         self.val_precisions.append(_val_precision)\n",
    "#         print(\"— val_f1: %f — val_precision: %f — val_recall %f\" %(_val_f1, _val_precision, _val_recall))\n",
    "#         return\n",
    "\n",
    "# metrics = Metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first compile"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# from keras import metrics"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional Xception layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "xcpetion_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy', fmeasure])\n",
    "#, metrics=[metrics.binary_accuracy,metrics.binary_crossentropy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train last layers of model (1st step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=results_path + 'weights.{epoch:02d}-{val_fmeasure:.2f}.hdf5', verbose=1)\n",
    "tensorBoard = TensorBoard(log_dir= results_path + r'tensorBoard/logs')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# train the model on the new data for a few epochs\n",
    "\n",
    "## with validation generator\n",
    "# history = xcpetion_model.fit_generator(train_gen, steps_per_epoch=big_train_steps_per_epoch,\n",
    "#                                      epochs=first_phase_epochs, verbose=1, callbacks=[checkpointer, tensorBoard],\n",
    "#                                      validation_data=val_gen, validation_steps=big_val_steps_per_epoch)\n",
    "\n",
    "# with validation dataset\n",
    "history = xcpetion_model.fit_generator(train_gen, steps_per_epoch=big_train_steps_per_epoch,\n",
    "                                     epochs=first_phase_epochs, verbose=1, callbacks=[checkpointer, tensorBoard],\n",
    "                                     validation_data=(x_test, y_test),\n",
    "                                      workers=1, use_multiprocessing=False)\n",
    "# ts = calendar.timegm(time.gmtime())\n",
    "xcpetion_model.save(results_path + 'first_phase' + '_xcpetion_model.h5')\n",
    "save_obj(history.history, results_path + 'first_phase' + '_xcpetion_history')\n",
    "    \n",
    "# for i in range(first_phase_train_reps):\n",
    "#     history = xcpetion_model.fit_generator(train_gen,\n",
    "#                                  steps_per_epoch=steps_per_small_epoch,\n",
    "#                                  epochs=small_epochs, verbose=1,\n",
    "#                                  validation_data=val_gen, validation_steps=val_steps_per_small_epoch)\n",
    "#     print(i)\n",
    "#     if i % saves_per_epoch == 0:\n",
    "#         print('{} epoch completed'.format(int(i / saves_per_epoch)))\n",
    "    \n",
    "#     ts = calendar.timegm(time.gmtime())\n",
    "#     xcpetion_model.save(save_path + str(ts) + '_xcpetion_model.h5')\n",
    "#     save_obj(history.history, save_path + str(ts) + '_xcpetion_history')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "out = xcpetion_model.predict(x_test)\n",
    "out = np.array(out)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "threshold = np.arange(0.1,0.9,0.1)\n",
    "\n",
    "f1 = []\n",
    "f1_scores = []\n",
    "best_threshold = np.zeros(out.shape[1])\n",
    "for i in range(out.shape[1]):\n",
    "    y_prob = np.array(out[:,i])\n",
    "    for j in threshold:\n",
    "        y_pred = [1 if prob>=j else 0 for prob in y_prob]\n",
    "        f1.append( f1_score(y_test[:,i], y_pred))\n",
    "    f1   = np.array(f1)\n",
    "    index = np.where(f1==f1.max()) \n",
    "    f1_scores.append(f1.max()) \n",
    "    best_threshold[i] = threshold[index[0][0]]\n",
    "    f1 = []\n",
    "    if i % 100 == 0:\n",
    "        print(i, 'out of', out.shape[1])\n",
    "\n",
    "y_pred = np.array([[1 if out[i,j]>=best_threshold[j] else 0 for j in range(y_test.shape[1])] for i in range(val_size)])\n",
    "save_obj(best_threshold, results_path + 'best_threshold')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "y_pred = np.array([[1 if out[i,j]>=.5 else 0 for j in range(y_test.shape[1])] for i in range(val_size)])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "f1 = 0\n",
    "for i in range(val_size):\n",
    "    f1 += f1_score(y_test[i,:], y_pred[i,:])\n",
    "f1 /= val_size\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### second compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xcpetion_model = load_model(results_path + 'first_phase' + '_xcpetion_model.h5', custom_objects={'fmeasure': fmeasure})\n",
    "#trainable_layers_ratio = 1 / 2.0\n",
    "#trainable_layers_index = int(len(xcpetion_model.layers) * (1 - trainable_layers_ratio))\n",
    "#for layer in xcpetion_model.layers[:trainable_layers_index]:\n",
    "#    layer.trainable = False\n",
    "#for layer in xcpetion_model.layers[trainable_layers_index:]:\n",
    "#    layer.trainable = True\n",
    "# for layer in xcpetion_model.layers[:]:\n",
    "#     layer.trainable = True\n",
    "\n",
    "xcpetion_model = load_model('/home/ubuntu/Dimitris/results/' + 'second_v2_phase_xcpetion_model.h5', custom_objects={'fmeasure': fmeasure})\n",
    "\n",
    "# predictions = xcpetion_model.layers[-1]\n",
    "# dropout = Dropout(0.3, name='fc_1024_dropout', seed=0)\n",
    "# fc = xcpetion_model.layers[-2]\n",
    "\n",
    "# x = dropout(fc.output)\n",
    "# predictors = predictions(x)\n",
    "# new_xcpetion_model = Model(inputs=xcpetion_model.input, outputs=predictors)\n",
    "\n",
    "# new_xcpetion_model.compile(optimizer=Adam(lr=0.0005), loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "xcpetion_model.compile(optimizer=Adam(lr=0.0005), loss='binary_crossentropy', metrics=[fmeasure])\n",
    "# model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fine tune half layers of model (2nd step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.fmeasures = []\n",
    "        self.val_fmeasures = []\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        with open('/home/ubuntu/Dimitris/results/' + 'logs_v3.csv', \"a\") as myfile:\n",
    "            myfile.write(','.join([\n",
    "                 \"\" if not logs.get('loss') else str(logs.get('loss')),\n",
    "                 \"\" if not logs.get('val_loss') else str(logs.get('val_loss')),\n",
    "                 \"\" if not logs.get('fmeasure') else str(logs.get('fmeasure')),\n",
    "                 \"\" if not logs.get('val_fmeasure') else str(logs.get('val_fmeasure'))\n",
    "            ]) + '\\n') \n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.fmeasures.append(logs.get('fmeasure'))\n",
    "        self.val_fmeasures.append(logs.get('val_fmeasure'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6867/6867 [==============================] - 4529s 660ms/step - loss: 9.9476e-04 - fmeasure: 0.2893 - val_loss: 0.0013 - val_fmeasure: 0.2243\n",
      "Epoch 2/10\n",
      "6867/6867 [==============================] - 4516s 658ms/step - loss: 9.6233e-04 - fmeasure: 0.3058 - val_loss: 0.0014 - val_fmeasure: 0.2232\n",
      "Epoch 3/10\n",
      "6867/6867 [==============================] - 4510s 657ms/step - loss: 9.2809e-04 - fmeasure: 0.3241 - val_loss: 0.0014 - val_fmeasure: 0.2324\n",
      "Epoch 4/10\n",
      "6867/6867 [==============================] - 4518s 658ms/step - loss: 8.9184e-04 - fmeasure: 0.3450 - val_loss: 0.0014 - val_fmeasure: 0.2301\n",
      "Epoch 5/10\n",
      "6867/6867 [==============================] - 4536s 661ms/step - loss: 8.5824e-04 - fmeasure: 0.3675 - val_loss: 0.0015 - val_fmeasure: 0.2338\n",
      "Epoch 6/10\n",
      "6867/6867 [==============================] - 4534s 660ms/step - loss: 8.2270e-04 - fmeasure: 0.3887 - val_loss: 0.0015 - val_fmeasure: 0.2300\n",
      "Epoch 7/10\n",
      "6867/6867 [==============================] - 4535s 660ms/step - loss: 7.8808e-04 - fmeasure: 0.4139 - val_loss: 0.0015 - val_fmeasure: 0.2301\n",
      "Epoch 8/10\n",
      "6867/6867 [==============================] - 4535s 660ms/step - loss: 7.5791e-04 - fmeasure: 0.4360 - val_loss: 0.0016 - val_fmeasure: 0.2354\n",
      "Epoch 9/10\n",
      "6867/6867 [==============================] - 4536s 660ms/step - loss: 7.2736e-04 - fmeasure: 0.4576 - val_loss: 0.0016 - val_fmeasure: 0.2312\n",
      "Epoch 10/10\n",
      "6867/6867 [==============================] - 4535s 660ms/step - loss: 6.9685e-04 - fmeasure: 0.4797 - val_loss: 0.0016 - val_fmeasure: 0.2248\n"
     ]
    }
   ],
   "source": [
    "# # train the model on the new data for a few epochs\n",
    "# for i in range(second_phase_train_reps):\n",
    "#     history = xcpetion_model.fit_generator(train_gen,\n",
    "#                                  steps_per_epoch=steps_per_small_epoch,\n",
    "#                                  epochs=small_epochs, verbose=2,\n",
    "#                                  validation_data=val_gen, validation_steps=val_steps_per_small_epoch)\n",
    "#     print(i)\n",
    "#     if i % saves_per_epoch == 0:\n",
    "#         print('{} epoch completed'.format(int(i / saves_per_epoch)))\n",
    "    \n",
    "#     ts = calendar.timegm(time.gmtime())\n",
    "#     xcpetion_model.save(save_path + str(ts) + '_xcpetion_model.h5')\n",
    "#     save_obj(history.history, save_path + str(ts) + '_xcpetion_history.h5')\n",
    "#checkpointer = ModelCheckpoint(filepath=results_path + 'second_v3_phase_weights.{epoch:02d}-{val_fmeasure:.2f}.hdf5', verbose=1)\n",
    "history = LossHistory()\n",
    "\n",
    "history = xcpetion_model.fit_generator(train_gen, steps_per_epoch=big_train_steps_per_epoch,\n",
    "                                       epochs=second_phase_epochs, verbose=1, callbacks=[tensorBoard, history, LosswiseKerasCallback(tag='keras xcpetion model for ImageCLEF')],\n",
    "                                       validation_data=(x_test, y_test), workers=1)\n",
    "# ts = calendar.timegm(time.gmtime())\n",
    "xcpetion_model.save(results_path + 'second_v3_phase' + '_xcpetion_model.h5')\n",
    "save_obj(history.history, results_path + 'second_v3_phase' + '_xcpetion_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
